{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import DataSets as ds\n",
    "import Layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 8)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class DataSets(object):\n",
    "\tdef __init__(self, filename_data, filename_gender, nbdata, L2normalize=False, batchSize=128):\n",
    "\t\tself.nbdata = nbdata\n",
    "\t\tself.name = filename_data\n",
    "\t\t# taille des images 48*48 pixels en niveau de gris\n",
    "\t\tself.dim = 2304\n",
    "\t\tself.data = None\n",
    "\t\tself.label = None\n",
    "\t\tself.batchSize = batchSize\n",
    "\t\tself.curPos = 0\n",
    "\n",
    "\t\tf = open(filename_data, 'rb')\n",
    "\t\tself.data = np.empty([nbdata, self.dim], dtype=np.float32)\n",
    "\t\tfor i in range(nbdata):\n",
    "\t\t\tself.data[i, :] = np.fromfile(f, dtype=np.uint8, count=self.dim)\n",
    "\t\tf.close()\n",
    "\n",
    "\t\tf = open(filename_gender, 'rb')\n",
    "\t\tself.label = np.empty([nbdata, 2], dtype=np.float32)\n",
    "\t\tfor i in range(nbdata):\n",
    "\t\t\tself.label[i, :] = np.fromfile(f, dtype=np.float32, count=2)\n",
    "\t\tf.close()\n",
    "\n",
    "\t\tprint('nb data = ', self.nbdata)\n",
    "\t\tself.data = (self.data - 128.0) / 256.0\n",
    "\n",
    "\t\ttmpdata = np.empty([1, self.dim], dtype=np.float32)\n",
    "\t\ttmplabel = np.empty([1, 2], dtype=np.float32)\n",
    "\t\tarr = np.arange(nbdata)\n",
    "\t\tnp.random.shuffle(arr)\n",
    "\t\ttmpdata = self.data[arr[0], :]\n",
    "\t\ttmplabel = self.label[arr[0], :]\n",
    "\t\tfor i in range(nbdata - 1):\n",
    "\t\t\tself.data[arr[i], :] = self.data[arr[i + 1], :]\n",
    "\t\t\tself.label[arr[i], :] = self.label[arr[i + 1], :]\n",
    "\t\tself.data[arr[nbdata - 1], :] = tmpdata\n",
    "\t\tself.label[arr[nbdata - 1], :] = tmplabel\n",
    "\t\tif L2normalize:\n",
    "\t\t\tself.data /= np.sqrt(np.expand_dims(np.square(self.data).sum(axis=1), 1))\n",
    "\t\t\n",
    "\tdef NextTrainingBatch(self):\n",
    "\t\tif self.curPos + self.batchSize > self.nbdata:\n",
    "\t\t\tself.curPos = 0\n",
    "\t\txs = self.data[self.curPos:self.curPos + self.batchSize, :]\n",
    "\t\tys = self.label[self.curPos:self.curPos + self.batchSize, :]\n",
    "\t\tself.curPos += self.batchSize\n",
    "\t\treturn xs, ys\n",
    "\n",
    "\tdef mean_accuracy(self, model):\n",
    "\t\tacc = 0\n",
    "\t\tfor i in range(0, self.nbdata, self.batchSize):\n",
    "\t\t\tcurBatchSize = min(self.batchSize, self.nbdata - i)\n",
    "\t\t\timages = self.data[i:i+curBatchSize,:]\n",
    "\t\t\tlabels = self.label[i:i+curBatchSize,:]\n",
    "\t\t\ty = model(images, False)\n",
    "\t\t\tcorrect_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(labels, 1))\n",
    "\t\t\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\t\t\tacc += accuracy * curBatchSize\n",
    "\t\tacc /= self.nbdata\n",
    "\t\ttf.summary.scalar('Accuracy %s'%self.name, acc)\n",
    "\t\treturn acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc(tf.Module):\n",
    "\tdef __init__(self, name, output_dim):\n",
    "\t\tself.scope_name = name\n",
    "\t\tself.output_dim = output_dim\n",
    "\t\tself.b = tf.Variable(tf.constant(0.0, shape=[self.output_dim]))\n",
    "\tdef __call__(self, x, log_summary):\n",
    "\t\tif not hasattr(self, 'w'):\n",
    "\t\t\tw_init = tf.random.truncated_normal([x.shape[1], self.output_dim], stddev=0.1)\n",
    "\t\t\tself.w = tf.Variable(w_init)\n",
    "\t\t\tprint('build fc %s  %d => %d' % (self.scope_name,  x.shape[1], self.output_dim))\n",
    "\n",
    "\t\tif log_summary:\n",
    "\t\t\twith tf.name_scope(self.scope_name) as scope:\n",
    "\t\t\t\ttf.summary.scalar(\"mean w\", tf.reduce_mean(self.w))\n",
    "\t\t\t\ttf.summary.scalar(\"max w\", tf.reduce_max(self.w))\n",
    "\t\t\t\ttf.summary.histogram(\"w\", self.w)\n",
    "\t\t\t\ttf.summary.scalar(\"mean b\", tf.reduce_mean(self.b))\n",
    "\t\t\t\ttf.summary.scalar(\"max b\", tf.reduce_max(self.b))\n",
    "\t\t\t\ttf.summary.histogram(\"b\", self.b)\n",
    "\t\treturn tf.matmul(x, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv(tf.Module):\n",
    "\tdef __init__(self, name, output_dim, filterSize, stride):\n",
    "\t\tself.scope_name = name\n",
    "\t\tself.filterSize = filterSize\n",
    "\t\tself.output_dim = output_dim\n",
    "\t\tself.stride = stride\n",
    "\t\tself.b = tf.Variable(tf.constant(0.0, shape=[self.output_dim]))\n",
    "\tdef __call__(self, x, log_summary):\n",
    "\t\tif not hasattr(self, 'w'):\n",
    "\t\t\tw_init = tf.random.truncated_normal([self.filterSize, self.filterSize, x.shape[3], self.output_dim], stddev=0.1)\n",
    "\t\t\tself.w = tf.Variable(w_init)\n",
    "\t\t\tprint('build conv %s %dx%d  %d => %d'%(self.scope_name,self.filterSize,self.filterSize, x.shape[3], self.output_dim))\n",
    "\t\tif log_summary:\n",
    "\t\t\twith tf.name_scope(self.scope_name) as scope:\n",
    "\t\t\t\ttf.summary.scalar(\"mean w\", tf.reduce_mean(self.w))\n",
    "\t\t\t\ttf.summary.scalar(\"max w\", tf.reduce_max(self.w))\n",
    "\t\t\t\ttf.summary.histogram(\"w\", self.w)\n",
    "\t\t\t\ttf.summary.scalar(\"mean b\", tf.reduce_mean(self.b))\n",
    "\t\t\t\ttf.summary.scalar(\"max b\", tf.reduce_max(self.b))\n",
    "\t\t\t\ttf.summary.histogram(\"b\", self.b)\n",
    "\t\tx = tf.nn.conv2d(x, self.w, strides=[1, self.stride, self.stride, 1], padding='SAME') + self.b\n",
    "\t\treturn tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class maxpool(tf.Module):\n",
    "\tdef __init__(self, name, poolSize):\n",
    "\t\tself.scope_name = name\n",
    "\t\tself.poolSize = poolSize\n",
    "\n",
    "\tdef __call__(self, x):\n",
    "\t\treturn tf.nn.max_pool2d(x, ksize=(1, self.poolSize, self.poolSize, 1),\n",
    "\t\t\t\t\t\t\t\tstrides=(1, self.poolSize, self.poolSize, 1), padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flat(tf.Module):\n",
    "\tdef __call__(self, x):\n",
    "\t\tinDimH = x.shape[1]\n",
    "\t\tinDimW = x.shape[2]\n",
    "\t\tinDimD = x.shape[3]\n",
    "\t\treturn tf.reshape(x, [-1, inDimH * inDimW * inDimD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class unflat(tf.Module):\n",
    "\tdef __init__(self, name, outDimH, outDimW, outDimD):\n",
    "\t\tself.scope_name = name\n",
    "\t\tself.new_shape = [-1, outDimH, outDimW, outDimD]\n",
    "\t\tprint('def unflat %s ? => %d %d %d' % (self.scope_name, outDimH, outDimW, outDimD))\n",
    "\n",
    "\tdef __call__(self, x, log_summary):\n",
    "\t\tx = tf.reshape(x, self.new_shape)\n",
    "\t\tif log_summary:\n",
    "\t\t\twith tf.name_scope(self.scope_name) as scope:\n",
    "\t\t\t\ttf.summary.image('input', x, 5)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb data =  100000\n",
      "nb data =  10000\n"
     ]
    }
   ],
   "source": [
    "LoadModel = True\n",
    "\n",
    "experiment_size = 100\n",
    "\n",
    "train = ds.DataSet('../Databases/data_%dk.bin'%experiment_size,'../Databases/gender_%dk.bin'%experiment_size,1000*experiment_size)\n",
    "\n",
    "test = ds.DataSet('../Databases/data_test10k.bin','../Databases/gender_test10k.bin',10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du r√©seau CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeuralNet(tf.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tself.unflat = Layers.unflat('unflat',48, 48, 1)\n",
    "\t\tself.cv1 = Layers.conv('conv_1', output_dim=3, filterSize=3, stride=1)\n",
    "\t\tself.mp = Layers.maxpool('pool', 2)\n",
    "\t\tself.cv2 = Layers.conv('conv_2', output_dim=6, filterSize=3, stride=1)\n",
    "\t\tself.cv3 = Layers.conv('conv_3', output_dim=12, filterSize=3, stride=1)\n",
    "\t\tself.flat = Layers.flat()\n",
    "\t\tself.fc = Layers.fc('fc', 2)\n",
    "\n",
    "\tdef __call__(self, x, log_summary):\n",
    "\t\tx = self.unflat(x, log_summary)\n",
    "\t\tx = self.cv1(x, log_summary)\n",
    "\t\tx = self.mp(x)\n",
    "\t\tx = self.cv2(x, log_summary)\n",
    "\t\tx = self.mp(x)\n",
    "\t\tx = self.cv3(x, log_summary)\n",
    "\t\tx = self.mp(x)\n",
    "\t\tx = self.flat(x)\n",
    "\t\tx = self.fc(x, log_summary)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_iter(model, optimizer, image, label, log_summary):\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\ty = model(image,log_summary)\n",
    "\t\ty = tf.nn.log_softmax(y)\n",
    "\t\tdiff = label * y\n",
    "\t\tloss = -tf.reduce_sum(diff)\n",
    "\t\tif log_summary:\n",
    "\t\t\ttf.summary.scalar('cross entropy', loss)\n",
    "\t\tgrads = tape.gradient(loss, model.trainable_variables)\n",
    "\t\toptimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "----------------------- 100k -------------------------\n",
      "-----------------------------------------------------\n",
      "def unflat unflat ? => 48 48 1\n"
     ]
    }
   ],
   "source": [
    "print (\"-----------------------------------------------------\")\n",
    "print (\"----------------------- %dk -------------------------\"%experiment_size)\n",
    "print (\"-----------------------------------------------------\")\n",
    "\n",
    "train_summary_writer = tf.summary.create_file_writer('logs %dk'%experiment_size)\n",
    "optimizer = tf.optimizers.Adam(1e-3)\n",
    "simple_cnn = ConvNeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LoadModel:\n",
    "\tckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, net=simple_cnn)\n",
    "\tckpt.restore('./saved_model-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=      0 accuracy - train= 81.37% - test= 80.28%\n",
      "iter=      0 - loss= 45.223297\n",
      "iter=    100 - loss= 358.860962\n",
      "iter=    200 - loss= 488.049591\n",
      "iter=    300 - loss= 86.234566\n",
      "iter=    400 - loss= 186.023407\n",
      "iter=    500 accuracy - train= 56.91% - test= 54.34%\n",
      "iter=    500 - loss= 570.728149\n",
      "iter=    600 - loss= 189.260757\n",
      "iter=    700 - loss= 227.979950\n",
      "iter=    800 - loss= 520.391602\n",
      "iter=    900 - loss= 505.286194\n"
     ]
    }
   ],
   "source": [
    "for iter in range(1000):\n",
    "\ttf.summary.experimental.set_step(iter)\n",
    "\n",
    "\tif iter % 500 == 0:\n",
    "\t\twith train_summary_writer.as_default():\n",
    "\t\t\tacc1 = train.mean_accuracy(simple_cnn) * 100\n",
    "\t\t\tacc2 = test.mean_accuracy(simple_cnn) * 100\n",
    "\t\t\tprint(\"iter= %6d accuracy - train= %.2f%% - test= %.2f%%\" % (iter, acc1, acc2))\n",
    "\n",
    "\tima, lab = train.NextTrainingBatch()\n",
    "\twith train_summary_writer.as_default():\n",
    "\t\tloss = train_one_iter(simple_cnn, optimizer, ima, lab, iter % 10 == 0)\n",
    "\tif iter % 100 == 0:\n",
    "\t\tprint(\"iter= %6d - loss= %f\" % (iter, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LoadModel:\n",
    "\tckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, net=simple_cnn)\n",
    "\tckpt.save('./saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=48x48 at 0x7F9A72203790>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ima_init = Image.open(\"homme.png\").convert('L')\n",
    "ima_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ckpt = tf.train.Checkpoint(step = tf.Variable(1), optimizer = optimizer, net = simple_cnn)\n",
    "#ckpt.restore('./saved_model-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5434>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.mean_accuracy(simple_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9996948e-01, 3.0501706e-05]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ima = np.array(ima_init,dtype=np.float32).reshape([1, 2304])\n",
    "ima = (ima - 128.0) / 256.0\n",
    "pred = tf.nn.softmax(simple_cnn(ima,False))\n",
    "pred.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mislead CNN detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2304), dtype=float32, numpy=\n",
       "array([[ 0.40625   ,  0.40625   ,  0.40625   , ..., -0.390625  ,\n",
       "        -0.40625   , -0.42578125]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.constant(ima)  #Creates a constant tensor from a tensor-like object.\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(1, 2304) dtype=float32, numpy=array([[0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DXinit = tf.constant(0.0, shape=[1,2304])\n",
    "DX = tf.Variable(DXinit)\n",
    "DX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load a wrong label\n",
    "wrong_label = [0.0,1.0]\n",
    "wrong_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a Variable DX to the input in your graph\n",
    "DXinit = tf.constant(0.0, shape=[1,2304])\n",
    "DX = tf.Variable(DXinit)\n",
    "Xmod = X + DX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train only the DX Variable\n",
    "tf.nn.log_softmax(simple_cnn(DX,False)).numpy()\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show Modified Image and DX Image\n",
    "modified_img = Image.fromarray(np.reshape((Xmod).numpy(),(48,48)))\n",
    "DX_img = Image.fromarray(np.reshape((DX).numpy(),(48,48)))\n",
    "\n",
    "modified_img.show()\n",
    "DX_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Modified Image. Xmod\n",
    "\n",
    "modified_img.convert('L').save(\"Xmod.png\")\n",
    "modified_img.convert('L').save(\"DX.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As DX is very small, build an amplified image.\n",
    "A = abs(DX)*50\n",
    "A_img = Image.fromarray(np.reshape(A.numpy(),(48,48)))\n",
    "A_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Modified Image with your small script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: [\"<tf.Variable 'Variable:0' shape=(1, 2304) dtype=float32, numpy=array([[0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>\"].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ab19f11dec59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrain_summary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise_one_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrong_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iter= %6d - loss= %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-ab19f11dec59>\u001b[0m in \u001b[0;36mnoise_one_iter\u001b[0;34m(model, optimizer, X, label)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m       raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0;32m--> 595\u001b[0;31m                        ([str(v) for _, v, _ in converted_grads_and_vars],))\n\u001b[0m\u001b[1;32m    596\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: [\"<tf.Variable 'Variable:0' shape=(1, 2304) dtype=float32, numpy=array([[0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>\"]."
     ]
    }
   ],
   "source": [
    "def noise_one_iter(model, optimizer, X,label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = model(Xmod,False)\n",
    "        y = tf.nn.log_softmax(y)\n",
    "        diff = label * y\n",
    "        loss = -tf.reduce_sum(diff)\n",
    "        grads = tape.gradient(loss,[DX]) \n",
    "        optimizer.apply_gradients(zip(grads, [DX]))\n",
    "    return loss\n",
    "\n",
    "for iter in range(1000):\n",
    "    with train_summary_writer.as_default():\n",
    "        loss = noise_one_iter(simple_cnn, optimizer, X, wrong_label)\n",
    "    if iter % 100 == 0:\n",
    "        print(\"iter= %6d - loss= %f\" % (iter, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an L2 constraint to DX to make it small\n",
    "tf.nn.l2_loss([DX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y = simple_cnn(X+DX,False)\n",
    "    y = tf.nn.log_softmax(y)\n",
    "    diff = wrong_label * y\n",
    "    loss = -tf.reduce_sum(diff)\n",
    "    grads = tape.gradient(loss,[DX]) \n",
    "    optimizer.apply_gradients(zip(grads, [DX]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "def create_adversarial_pattern(input_image, input_label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_image)\n",
    "        prediction = pretrained_model(input_image)\n",
    "        loss = loss_object(input_label, prediction)\n",
    "\n",
    "    # Get the gradients of the loss w.r.t to the input image.\n",
    "    gradient = tape.gradient(loss, input_image)\n",
    "    # Get the sign of the gradients to create the perturbation\n",
    "    signed_grad = tf.sign(gradient)\n",
    "    return signed_grad\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
